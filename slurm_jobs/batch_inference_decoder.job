#!/bin/bash
#SBATCH --job-name=qwen3_batch
#SBATCH --partition=gpu_course
#SBATCH --gpus-per-node=1
#SBATCH --ntasks=1
#SBATCH --mem=8G
#SBATCH --cpus-per-task=2
#SBATCH --time=00:20:00
#SBATCH --output=logs/vllm_batch_decoding_%j.log
#SBATCH --error=logs/vllm_batch_decoding_%j.err

set -euo pipefail

# --- Parameters ---
export PORT=$(shuf -i 10000-60000 -n 1)   # random port
export CONTAINER_PATH="/projects/2/managed_datasets/containers/vllm/cuda-13.0-vllm.sif"
MODEL="Qwen/Qwen3-4B-AWQ"
MODE="no_think"
CONCURRENCY=8

# --- Choose your input/output ---
# Mini-test:
INPUT_FILE="./data/processed/text/v1/decoder_input_test_fresh.jsonl"
OUTPUT_FILE="./data/processed/text/v1/decoder_output_test_fresh.jsonl"

# Full run:
# INPUT_FILE="./data/processed/text/v1/decoder_input_train.jsonl"
# OUTPUT_FILE="./data/processed/text/v1/decoder_output_train.jsonl"

# Check folders exist
mkdir -p "$(dirname "$OUTPUT_FILE")"

echo "[INFO] NODE=$(hostname)"
echo "[INFO] INPUT_FILE=$INPUT_FILE"
echo "[INFO] OUTPUT_FILE=$OUTPUT_FILE"
echo "[INFO] PORT=$PORT"

# --- Start vLLM Server in background ---
apptainer exec --nv -B "$PWD" "$CONTAINER_PATH" \
  vllm serve "$MODEL" \
    --host 127.0.0.1 \
    --port $PORT \
    --max-num-seqs $CONCURRENCY \
    --max-model-len 2048 \
    --gpu-memory-utilization 0.90 &

SERVER_PID=$!
echo "[INFO] vLLM server PID=$SERVER_PID"

# --- Wait until server is ready ---
echo "[INFO] Waiting for vLLM server to be ready..."
while ! curl -s "http://127.0.0.1:$PORT/v1/models" > /dev/null; do
    if ! kill -0 $SERVER_PID 2>/dev/null; then
        echo "[ERROR] vLLM server failed to start."
        exit 1
    fi
    sleep 2
done
echo "[INFO] vLLM server is ready!"

# --- Run decoder ---
apptainer exec --nv -B "$PWD" "$CONTAINER_PATH" \
  python3 python_scripts/batch_inference_decoder.py \
    --input "$INPUT_FILE" \
    --output "$OUTPUT_FILE" \
    --port $PORT \
    --mode "$MODE" \
    --concurrency $CONCURRENCY

# --- Stop server ---
kill $SERVER_PID || true
sleep 2
echo "[INFO] Done."
