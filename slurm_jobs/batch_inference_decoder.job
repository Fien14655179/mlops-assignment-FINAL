#!/bin/bash
#SBATCH --job-name=qwen3_batch
#SBATCH --partition=gpu_course
#SBATCH --gpus-per-node=1
#SBATCH --ntasks=1
#SBATCH --mem=16G
#SBATCH --cpus-per-task=2
#SBATCH --time=00:20:00
#SBATCH --output=logs/vllm_batch_decoding_%j.log
#SBATCH --error=logs/vllm_batch_decoding_%j.err

set -euo pipefail

export PORT=$(shuf -i 10000-60000 -n 1)
export CONTAINER_PATH="/projects/2/managed_datasets/containers/vllm/cuda-13.0-vllm.sif"
MODEL="Qwen/Qwen3-4B-AWQ"

CONCURRENCY=8
MAX_TOKENS=1024

INPUT_FILE="./data/processed/text/v1/decoder_input_test.jsonl"
OUTPUT_FILE="./data/processed/text/v1/decoder_output_test_v3.jsonl"

mkdir -p logs
mkdir -p "$(dirname "$OUTPUT_FILE")"

echo "[INFO] NODE=$(hostname)"
echo "[INFO] INPUT_FILE=$INPUT_FILE"
echo "[INFO] OUTPUT_FILE=$OUTPUT_FILE"
echo "[INFO] PORT=$PORT"

apptainer exec --nv -B "$PWD" "$CONTAINER_PATH" \
  vllm serve "$MODEL" \
    --host 127.0.0.1 \
    --port $PORT \
    --max-num-seqs $CONCURRENCY \
    --max-model-len 2048 \
    --gpu-memory-utilization 0.90 &

SERVER_PID=$!
echo "[INFO] vLLM server PID=$SERVER_PID"

echo "[INFO] Waiting for vLLM server..."
while ! curl -s "http://127.0.0.1:$PORT/v1/models" > /dev/null; do
    if ! kill -0 $SERVER_PID 2>/dev/null; then
        echo "[ERROR] vLLM server failed to start."
        exit 1
    fi
    sleep 2
done
echo "[INFO] vLLM server is ready!"

apptainer exec --nv -B "$PWD" "$CONTAINER_PATH" \
  python3 python_scripts/batch_inference_decoder.py \
    --input "$INPUT_FILE" \
    --output "$OUTPUT_FILE" \
    --port $PORT \
    --concurrency $CONCURRENCY \
    --max_tokens $MAX_TOKENS

kill $SERVER_PID || true
sleep 2
echo "[INFO] Done."
